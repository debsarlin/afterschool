{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DVLab-Workshop-2021-03-24.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debsarlin/afterschool/blob/master/DVLab_Workshop_2021_03_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ySD47jGzAz"
      },
      "source": [
        "**Images as Data with the Distant Viewing Toolkit**\n",
        "\n",
        "*Date*: 24 March 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc3eXbSiWL10"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "This document displays a file known as a Jupyter notebook. In this case, it contains a mix of plain text (like this one!) and code in the open-source programing language Python. The notebook is being hosted on Google's Colab platform, which allows us to run the code for free on a third-party system without the need to install Python and its many dependencies on our local machine. If you are interested in using these methods further, however, it should be possible to install all of this on your machine and run the code locally.\n",
        "\n",
        "In this tutorial we will demonstrate two different computer vision algorithms using a subset of functions available from the Distant Viewing Toolkit. We will look at a subset of 1000 images from Harvard University Art Museum's *American Professional Photographers* collection. This collection was assembled by Barbara Norfleet from 1975-1977 from forgotten negatives and prints held by over 25 professional photography studios. The photographs focus on the depiction of social events — such as weddings, parties, and beauty pageants — that the studios were hired to photograph. The entire collection can be found at [harvardartmuseums.org](https://harvardartmuseums.org/collections?q=%22American+Professional+Photographers+Collection%22).\n",
        "\n",
        "To run the code below, all that is needed is to sign into a Google account (you will be prompted to login by clicking on the first code block below). For more information about this project, please see the [Distant Viewing Lab](https://www.distantviewing.org/), the [Collections as Data Website](https://collectionsasdata.github.io/), and the References section at the bottom of this page. Questions or issues with the code can be sent to Taylor Arnold (tarnold2@richmond.edu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# 2. Install Python Modules\n",
        "\n",
        "The Google Colab environment already has a running version of Python and several of the most common modules (third-party code that extends the basic language). We only need to install a few other pieces, such as Detectron2 and the Distant Viewing workshop code. To do this, hover over the code block below and click on the run button that appears in the upper left corner of the block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR"
      },
      "source": [
        "!pip install -q pyyaml==5.1\n",
        "!pip install -q Keras tensorflow\n",
        "!pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "!pip install -q git+https://github.com/distant-viewing/dvt_workshop.git\n",
        "# exit(0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26J3lzTJlK_"
      },
      "source": [
        "It is possible that you may see one or two errors about extra dependencies. Our experience is that these can be ignored for the moment. However, before continuing, it is important that you first restart the Python runtime to properly finish the installation of the above components. To do this, click on the menu above and select the option **Runtime > Restart runtime**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_9cw-bZ40NF"
      },
      "source": [
        "\n",
        "# 3. Download Data \n",
        "\n",
        "Now that we have the Python modules installed, we next need to  download the image data and metadata. This can be done by running the following code block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPp9mIvv48Ug"
      },
      "source": [
        "!wget https://distantviewing.org/appc_workshop_set.tar.gz -q -O appc_workshop_set.tar.gz\n",
        "!wget https://distantviewing.org/appc_workshop_meta.csv -q -O appc_workshop_meta.csv\n",
        "!tar -xf appc_workshop_set.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATjhjJmpRq4v"
      },
      "source": [
        "Once finished, you will have a CSV file containing metadata about the collection as well as the 1000 digitized images that we will look at in this tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvyRhgnZ48oA"
      },
      "source": [
        "# 4. Load Modules\n",
        "\n",
        "As a final setup task, we need to load in all of the libraries that we will use in the workshop. Again, just run the following lines to load the modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "source": [
        "# import some common libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from os.path import join\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import functions from dvt_workshop\n",
        "from dvt_workshop import (\n",
        "    dvt_detectron_config,\n",
        "    dvt_tidy_instance_data,\n",
        "    dvt_show_instance_predictions,\n",
        "    dvt_load_embed_image_model,\n",
        "    dvt_embed_image\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yae6n_srSH5l"
      },
      "source": [
        "Note that you will ideally not see any output in the previous block, which means that everything loaded without a problem. If you did have an error, this is likely an indication that a more serious problem has occured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4gID50K03a"
      },
      "source": [
        "# 5. Load Metadata\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "Now, let's load the metadata about the collection into Python. We do this using the function `pd.read_csv` and providing the name of the file that we downloaded above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrDiPPPlBTDF"
      },
      "source": [
        "meta = pd.read_csv(\"appc_workshop_meta.csv\")\n",
        "meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrN9i4kfSn_E"
      },
      "source": [
        "There is limited metadata here. Most importantly for us is the variable called *path*, which gives the filename associated with the image. We also see an indication of the date (note: the date is approximate) of when the image was taken and an indication of whether this is a digized negative or a digitized print."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKyUL4pngvE"
      },
      "source": [
        "# 6. Looking at an Image in Python\n",
        "\n",
        "Let's grab an image from the corpus and display it in Python. To do this, we use the functions `cv2.imread` and `cv_imshow`. The code is written to grab row 10's image. You can change the number to anything between 0 and 999 to see a different image. (Note: If you have an error when running the code below, it is likely because you forgot to reset the runtime after running Section 2. To fix, reset the runtime and run the code in Sections 3-5 again.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9GY37ml1kr"
      },
      "source": [
        "im = cv2.imread(join(\"appc_workshop_set\", meta.path[10]))\n",
        "cv2_imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAHy2mlwTS4a"
      },
      "source": [
        "Note that digital images are stored in Python as three large grids of numbers. These correspond to the red, green, and blue intensities of each pixel in the image.\n",
        "\n",
        "\n",
        "\n",
        "We can see the size of the image by looking at the shape attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1-oeVLOjWR"
      },
      "source": [
        "im.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfx6dC7UTkHd"
      },
      "source": [
        "This image is 390 pixels high, 521 pixels wide, and has three color *channels* (red, green, and blue). The image is black and white, but technically coded as a color image that consists entirely of shades of gray. The original image has a higher resolution; we have created a smaller version here to make the code run quickly during the workshop.\n",
        "\n",
        "We can look at the first few rows and columns of the image. Here is the upper left-hand corner of the image's red channel intensities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oN-9-zQRtZI"
      },
      "source": [
        "im[:10, :10, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_rCJZdUPvB"
      },
      "source": [
        "A value of 0 indicates that the pixel is entirely black and a value of 255 indicates white (at least, when we know that a color is greyscale; color images are more difficult to directly describe directly). Looking at the pixels, we can see the black border of the image as the first four rows of pixels, followed by the relatively bright sky below it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Ax6yk6D3LP"
      },
      "source": [
        "# 7. Running Object Segmentation\n",
        "\n",
        "Now, let's do some computer vision with the data, starting with image segmentation. The thing-stuff segmentation does not work well with grayscale images, but detecting objects does. We will load this model using the function `dvt_detectron_config` (it will automatically download some additional data the first time it is run)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhX19bWsCVkn"
      },
      "source": [
        "dvt_image_segment, md = dvt_detectron_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmSdn2xTVHfX"
      },
      "source": [
        "Once we have the model loaded, we will run the function `dvt_image_segment` to detect objects and their locations within the image. Our function `dvt_tidy_instance_data` collects the objects into a rectangular dataset. We see that the algorithm has detected a number of different people, which matches our understanding of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUjkwRsOn1O0"
      },
      "source": [
        "instances = dvt_image_segment(im)\n",
        "dvt_tidy_instance_data(instances, md, meta.path[7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnuOI9QaN_ry"
      },
      "source": [
        "We can further visualize the predictions by drawing the detected objects over the image with the function `dvt_show_instance_predictions`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRGo8d0qkgR"
      },
      "source": [
        "dvt_show_instance_predictions(instances, md, im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESBg26vXVj8O"
      },
      "source": [
        "And we see that the algorithm does a very good job of detecting and locating the people in a relatively old (compared to the data used to train the model) and low-resolution image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpTK1VCCEKqb"
      },
      "source": [
        "# 8. Image Segmentation Across the Corpus\n",
        "\n",
        "Now that we know how to detect people and objects within an image, let's loop over the entire collection to detect objects in all 1000 images. The following code runs the same code as above, but over every row of the metadata. It prints out a running summary after every 25 images to indicate that it is still working well. The runtime depends a bit on how many people are using the Google servers, but usually finishes in just a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i50PiKCK9ld"
      },
      "source": [
        "df = []\n",
        "for iter, path in enumerate(meta.path[1:]):\n",
        "    im = cv2.imread(join(\"appc_workshop_set\", path))\n",
        "    instances = dvt_image_segment(im)\n",
        "    df.append(dvt_tidy_instance_data(instances, md, path))\n",
        "    if (iter % 25) == 0:\n",
        "      print(\"Done with {0:d} of {1:d}\".format(iter, meta.shape[0]))\n",
        "\n",
        "\n",
        "df = pd.concat(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJP67p5OWIwn"
      },
      "source": [
        "Looking at the output, we see that there a large number of people and objects that have been detected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIJFnVp4Fkqg"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj8WytLfWOtx"
      },
      "source": [
        "Just to get a sense of what has been detected, let's find those images that contain a high number of certain objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqCjpTDvNm8-"
      },
      "source": [
        "df.value_counts(subset=[\"path\", \"class\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlmM5jBOWZEt"
      },
      "source": [
        "We can then look at the results. For example, this image of a ship with dozens of sailors looking over the railing has 42 people."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkyCZA6cOCua"
      },
      "source": [
        "im = cv2.imread(join(\"appc_workshop_set\", \"137348_42928874_INV010704P.jpeg\"))\n",
        "instances = dvt_image_segment(im)\n",
        "dvt_show_instance_predictions(instances, md, im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajV9tWPWjdt"
      },
      "source": [
        "We can then look at another image of a woman sitting at a desk. We detected 36 books on the bookshelves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1_V0q7qOqO8"
      },
      "source": [
        "im = cv2.imread(join(\"appc_workshop_set\", \"122462_20428547_INV165733P.jpeg\"))\n",
        "instances = dvt_image_segment(im)\n",
        "dvt_show_instance_predictions(instances, md, im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txyNOwoFWtUP"
      },
      "source": [
        "Take a moment to look at what was and was not detected in these images. Notice that the algorithms are quite good at detecting people, but struggle more with other kinds of objects (did you see the \"mouse\" on the desk in the second image?). We tend to use it most for detecting people, and even then do so with the understanding that it will both miss some people in the images and mistakenly detect people in others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkWcZuWLElRO"
      },
      "source": [
        "# 9. Image Embeddings\n",
        "\n",
        "As a second model, let's run an image embedding algorithm. This associates an image with a string of numbers. The specific numbers do not have an intrinsic meaning, but images with similar values will share common features. To start, we load the embedding model with the function `dvt_load_embed_image_model`. As with the previous model, this will download additional parts of the model the first time it is run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LiCJOe3Fn1I"
      },
      "source": [
        "dvt_embed = dvt_load_embed_image_model(\"fc1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzeVN3oMXhAy"
      },
      "source": [
        "To compute the embedding, we run the function `dvt_embed_image`, which returns the sequence of numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyIO-VZ0Es1R"
      },
      "source": [
        "em = dvt_embed_image(join(\"appc_workshop_set\", meta.path[22]), dvt_embed)\n",
        "em"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l2ZnBazXrCx"
      },
      "source": [
        "Looking at the shape of the output, we see that the embedding assigns 4096 numbers (2^12) to the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVewfmHSXqMx"
      },
      "source": [
        "em.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luVWDe7HXpfl"
      },
      "source": [
        "Embeddings are not very interesting on their own. Let's compute the embedding for the entire corpus by looping over each row of our metadata. As with the example above, this will print out the progress after every 25 images, and it takes a couple of minutes on average to finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEUkw_V5F0Mc"
      },
      "source": [
        "em_list = []\n",
        "for iter, path in enumerate(meta.path):\n",
        "    em = dvt_embed_image(join(\"appc_workshop_set\", path), dvt_embed)\n",
        "    em_list.append(em)\n",
        "    if (iter % 25) == 0:\n",
        "        print(\"Done with {0:d} of {1:d}\".format(iter, meta.shape[0]))\n",
        "\n",
        "\n",
        "em = np.vstack(em_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdgr912OYLHS"
      },
      "source": [
        "The output now has 1000 rows (one for each image) and 4096 columns (to contain the embedding for each image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZebCvjqmYN7-"
      },
      "source": [
        "em.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVk4x2QhYOQC"
      },
      "source": [
        "Finally, we can use these values to see which images in the collection are most similar to one another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R7FXqugQl5K"
      },
      "source": [
        "# 10. Recommendation System\n",
        "\n",
        "For this last section, let's build a basic recommendation system for our collection of images. Given a starting image, this system will find other images in the corpus that are the other most similar in terms of their embedding. To start, let's set up the plot area on Python to display an array of images all at once by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89zFeLprP7gB"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (16,12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt0TCaabY1YZ"
      },
      "source": [
        "Now, the code below selects an image by its row number and prints out the ten images that have the most similar embedding. Each image will always be closest to itself, therefore the image in the upper left of the output is the image we are comparing to. You should experiment with setting the variable `ref_img_num` to different values between 0 and 999 to see how the algorithm works for different starting images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K47ncZK1PN-E"
      },
      "source": [
        "ref_img_num = 612\n",
        "idx = np.argsort(np.sum(np.abs(em - em[ref_img_num, :])**2, axis=1))[:9]\n",
        "\n",
        "for ind, i in enumerate(idx):\n",
        "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "    plt.subplot(3, 3, ind + 1)\n",
        "\n",
        "    img = cv2.imread(join(\"appc_workshop_set\", meta.path.values[i]))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHQuzYhaZLd0"
      },
      "source": [
        "Hopefully you can see some interesting patterns that are found in the data. We found that it tends to group images based on the number of people in the image and their relative sizes within the camera's frame. It also detects some common objects. We admit that not all of results are perfect, with some images not having much in common with one another. This is due to the relatively small size of this subset of the collection. When run on larger collections there is more room for each image to be very similar to others. You can see an example of this in action on our digital public project [Photogrammar](https://photogrammar.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu_dCTVJG8N6"
      },
      "source": [
        "# References\n",
        "\n",
        "For more information about our work and projects, please see the following papers and projects:\n",
        "\n",
        "- T. Arnold, P. Leonard, and L. Tilton, (2017). \"Knowledge creation through recommender systems.\" *Digital Scholarship in the Humanities*, Volume 32, Issue Supplement_2, December 2019, Pages ii151–ii157.\n",
        " [DOI: 10.1093/llc/fqx035](https://doi.org/10.1093/llc/fqx035).\n",
        "- T. Arnold and L. Tilton, (2019). \"Distant Viewing: Analyzing Large Visual Corpora.\" *Digital Scholarship in the Humanities*, Volume 34, Issue Supplement_1, December 2019, Pages i3–i16.\n",
        " [DOI: 10.1093/llc/fqz013](https://doi.org/10.1093/llc/fqz013).\n",
        "- T. Arnold, A. Berke, and L. Tilton, (2019). \"Visual Style in Two Network Era Sitcoms\". *Journal of Cultural Analytics*. [DOI: 10.22148/16.043](https://doi.org/10.22148/16.043)\n",
        "- T. Arnold and L. Tilton, (2020). \"Distant Viewing Toolkit: A Python Package for the Analysis of Visual Culture\". *Journal of Open Source Software*, 5(45), 1800, [DOI: 10.21105/joss.01800](https://doi.org/10.21105/joss.01800)\n",
        "- T. Arnold, N. Ayers, J. Madron, R. Nelson, L.Tilton, L. Wexler. (2021) [Photogrammar](https://photogrammar.org) (Version 3.0). 2021.\n"
      ]
    }
  ]
}